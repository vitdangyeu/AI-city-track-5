{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7878153,"sourceType":"datasetVersion","datasetId":4623589},{"sourceId":7906848,"sourceType":"datasetVersion","datasetId":4595378}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:35:02.226716Z","iopub.execute_input":"2024-03-21T15:35:02.227435Z","iopub.status.idle":"2024-03-21T15:35:08.494393Z","shell.execute_reply.started":"2024-03-21T15:35:02.227403Z","shell.execute_reply":"2024-03-21T15:35:08.493626Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## **Model and Transform**","metadata":{}},{"cell_type":"code","source":"# Load pre-trained ResNet-50\ndevice = torch.device('cuda')\nresnet = models.resnet50(weights=False)\nresnet = nn.Sequential(*list(resnet.children())[:-1])\nmlp = nn.Sequential(\n    nn.Linear(2048, 2),\n)\n# Kết hợp ResNet và MLP\nclass CombinedModel(nn.Module):\n    def __init__(self, encoder, head):\n        super(CombinedModel, self).__init__()\n        self.encoder = encoder\n        self.head = head\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = x.view(x.size(0), -1)\n        x = self.head(x)\n        return x\n\n# Tạo mô hình hoàn chỉnh\nmodel = CombinedModel(resnet, mlp)\n\n# Tải mô hình từ tệp nhị phân\nmodel_path = '/kaggle/input/resnet-dataset/pytorch_model.bin'\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:35:08.495780Z","iopub.execute_input":"2024-03-21T15:35:08.496323Z","iopub.status.idle":"2024-03-21T15:35:11.782598Z","shell.execute_reply.started":"2024-03-21T15:35:08.496298Z","shell.execute_reply":"2024-03-21T15:35:11.781756Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"CombinedModel(\n  (encoder): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (5): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (6): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (6): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (7): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (8): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (9): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (10): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (11): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (12): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (13): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (14): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (15): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (16): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (17): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (18): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (19): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (20): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (21): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (22): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (7): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n  (head): Sequential(\n    (0): Linear(in_features=2048, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Transform\ntransform = transforms.Compose([\n        transforms.Resize((150,150),interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5])\n        #transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n        ])","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:35:11.783791Z","iopub.execute_input":"2024-03-21T15:35:11.784136Z","iopub.status.idle":"2024-03-21T15:35:11.789399Z","shell.execute_reply.started":"2024-03-21T15:35:11.784106Z","shell.execute_reply":"2024-03-21T15:35:11.788558Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def pred_1_sample(root_dir, data, model, transform, device):\n    image_path = os.path.join(root_dir,'0' * (3-len(str(line[0]))) + f\"{line[0]}\", f\"frame{line[1]-1}.jpg\")\n    image = Image.open(image_path)\n    cropped_image = image.crop((line[2], line[3], line[2]+line[4], line[3]+line[5]))\n    image_tensor = transform(cropped_image)\n    image_batch = image_tensor.unsqueeze(0)\n    #Pred\n    pred = model(image_batch.to(device))\n    score = F.softmax(pred, dim=1)\n    output = torch.argmax(score, 1)\n    return round(score[0,output].item(),2), output.item()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:35:11.791815Z","iopub.execute_input":"2024-03-21T15:35:11.792191Z","iopub.status.idle":"2024-03-21T15:35:11.800799Z","shell.execute_reply.started":"2024-03-21T15:35:11.792137Z","shell.execute_reply":"2024-03-21T15:35:11.799902Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"gt_dir = '/kaggle/input/resnet-dataset/gt_pred_yolo5.txt'\nroot_dir = '/kaggle/input/aicity-track5-test'\ndata = []\ni = 0\nwith open(gt_dir, \"r\") as file:\n    for line in file:\n        line = line.strip().split(',')\n        line[:7] = [int(x) for x in line[:7]]\n        line[7] = float(line[7])\n        if line[6] == 1:\n            data.append(line)\n        elif line[6] <= 5:\n            score, label = pred_1_sample(root_dir, line, model, transform, device)\n            line[7] = line[7] * 0.85 + score * 0.15\n            if label == 0:\n                line[6] = line[6] * 2 - 1\n            else:\n                line[6] = line[6] * 2 - 2\n            data.append(line)\n            \n        i = i+1\n        if(i%100==0): print(f\"Done {i} sample\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:35:11.801964Z","iopub.execute_input":"2024-03-21T15:35:11.802316Z","iopub.status.idle":"2024-03-21T15:55:25.869583Z","shell.execute_reply.started":"2024-03-21T15:35:11.802284Z","shell.execute_reply":"2024-03-21T15:55:25.868658Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Done 100 sample\nDone 200 sample\nDone 300 sample\nDone 400 sample\nDone 500 sample\nDone 600 sample\nDone 700 sample\nDone 800 sample\nDone 900 sample\nDone 1000 sample\nDone 1100 sample\nDone 1200 sample\nDone 1300 sample\nDone 1400 sample\nDone 1500 sample\nDone 1600 sample\nDone 1700 sample\nDone 1800 sample\nDone 1900 sample\nDone 2000 sample\nDone 2100 sample\nDone 2200 sample\nDone 2300 sample\nDone 2400 sample\nDone 2500 sample\nDone 2600 sample\nDone 2700 sample\nDone 2800 sample\nDone 2900 sample\nDone 3000 sample\nDone 3100 sample\nDone 3200 sample\nDone 3300 sample\nDone 3400 sample\nDone 3500 sample\nDone 3600 sample\nDone 3700 sample\nDone 3800 sample\nDone 3900 sample\nDone 4000 sample\nDone 4100 sample\nDone 4200 sample\nDone 4300 sample\nDone 4400 sample\nDone 4500 sample\nDone 4600 sample\nDone 4700 sample\nDone 4800 sample\nDone 4900 sample\nDone 5000 sample\nDone 5100 sample\nDone 5200 sample\nDone 5300 sample\nDone 5400 sample\nDone 5500 sample\nDone 5600 sample\nDone 5700 sample\nDone 5800 sample\nDone 5900 sample\nDone 6000 sample\nDone 6100 sample\nDone 6200 sample\nDone 6300 sample\nDone 6400 sample\nDone 6500 sample\nDone 6600 sample\nDone 6700 sample\nDone 6800 sample\nDone 6900 sample\nDone 7000 sample\nDone 7100 sample\nDone 7200 sample\nDone 7300 sample\nDone 7400 sample\nDone 7500 sample\nDone 7600 sample\nDone 7700 sample\nDone 7800 sample\nDone 7900 sample\nDone 8000 sample\nDone 8100 sample\nDone 8200 sample\nDone 8300 sample\nDone 8400 sample\nDone 8500 sample\nDone 8600 sample\nDone 8700 sample\nDone 8800 sample\nDone 8900 sample\nDone 9000 sample\nDone 9100 sample\nDone 9200 sample\nDone 9300 sample\nDone 9400 sample\nDone 9500 sample\nDone 9600 sample\nDone 9700 sample\nDone 9800 sample\nDone 9900 sample\nDone 10000 sample\nDone 10100 sample\nDone 10200 sample\nDone 10300 sample\nDone 10400 sample\nDone 10500 sample\nDone 10600 sample\nDone 10700 sample\nDone 10800 sample\nDone 10900 sample\nDone 11000 sample\nDone 11100 sample\nDone 11200 sample\nDone 11300 sample\nDone 11400 sample\nDone 11500 sample\nDone 11600 sample\nDone 11700 sample\nDone 11800 sample\nDone 11900 sample\nDone 12000 sample\nDone 12100 sample\nDone 12200 sample\nDone 12300 sample\nDone 12400 sample\nDone 12500 sample\nDone 12600 sample\nDone 12700 sample\nDone 12800 sample\nDone 12900 sample\nDone 13000 sample\nDone 13100 sample\nDone 13200 sample\nDone 13300 sample\nDone 13400 sample\nDone 13500 sample\nDone 13600 sample\nDone 13700 sample\nDone 13800 sample\nDone 13900 sample\nDone 14000 sample\nDone 14100 sample\nDone 14200 sample\nDone 14300 sample\nDone 14400 sample\nDone 14500 sample\nDone 14600 sample\nDone 14700 sample\nDone 14800 sample\nDone 14900 sample\nDone 15000 sample\nDone 15100 sample\nDone 15200 sample\nDone 15300 sample\nDone 15400 sample\nDone 15500 sample\nDone 15600 sample\nDone 15700 sample\nDone 15800 sample\nDone 15900 sample\nDone 16000 sample\nDone 16100 sample\nDone 16200 sample\nDone 16300 sample\nDone 16400 sample\nDone 16500 sample\nDone 16600 sample\nDone 16700 sample\nDone 16800 sample\nDone 16900 sample\nDone 17000 sample\nDone 17100 sample\nDone 17200 sample\nDone 17300 sample\nDone 17400 sample\nDone 17500 sample\nDone 17600 sample\nDone 17700 sample\nDone 17800 sample\nDone 17900 sample\nDone 18000 sample\nDone 18100 sample\nDone 18200 sample\nDone 18300 sample\nDone 18400 sample\nDone 18500 sample\nDone 18600 sample\nDone 18700 sample\nDone 18800 sample\nDone 18900 sample\nDone 19000 sample\nDone 19100 sample\nDone 19200 sample\nDone 19300 sample\nDone 19400 sample\nDone 19500 sample\nDone 19600 sample\nDone 19700 sample\nDone 19800 sample\nDone 19900 sample\nDone 20000 sample\nDone 20100 sample\nDone 20200 sample\nDone 20300 sample\nDone 20400 sample\nDone 20500 sample\nDone 20600 sample\nDone 20700 sample\nDone 20800 sample\nDone 20900 sample\nDone 21000 sample\nDone 21100 sample\nDone 21200 sample\nDone 21300 sample\nDone 21400 sample\nDone 21500 sample\nDone 21600 sample\nDone 21700 sample\nDone 21800 sample\nDone 21900 sample\nDone 22000 sample\nDone 22100 sample\nDone 22200 sample\nDone 22300 sample\nDone 22400 sample\nDone 22500 sample\nDone 22600 sample\nDone 22700 sample\nDone 22800 sample\nDone 22900 sample\nDone 23000 sample\nDone 23100 sample\nDone 23200 sample\nDone 23300 sample\nDone 23400 sample\nDone 23500 sample\nDone 23600 sample\nDone 23700 sample\nDone 23800 sample\nDone 23900 sample\nDone 24000 sample\nDone 24100 sample\nDone 24200 sample\nDone 24300 sample\nDone 24400 sample\nDone 24500 sample\nDone 24600 sample\nDone 24700 sample\nDone 24800 sample\nDone 24900 sample\nDone 25000 sample\nDone 25100 sample\nDone 25200 sample\nDone 25300 sample\nDone 25400 sample\nDone 25500 sample\nDone 25600 sample\nDone 25700 sample\nDone 25800 sample\nDone 25900 sample\nDone 26000 sample\nDone 26100 sample\nDone 26200 sample\nDone 26300 sample\nDone 26400 sample\nDone 26500 sample\nDone 26600 sample\nDone 26700 sample\nDone 26800 sample\nDone 26900 sample\nDone 27000 sample\nDone 27100 sample\nDone 27200 sample\nDone 27300 sample\nDone 27400 sample\nDone 27500 sample\nDone 27600 sample\nDone 27700 sample\nDone 27800 sample\nDone 27900 sample\nDone 28000 sample\nDone 28100 sample\nDone 28200 sample\nDone 28300 sample\nDone 28400 sample\nDone 28500 sample\nDone 28600 sample\nDone 28700 sample\nDone 28800 sample\nDone 28900 sample\nDone 29000 sample\nDone 29100 sample\nDone 29200 sample\nDone 29300 sample\nDone 29400 sample\nDone 29500 sample\nDone 29600 sample\nDone 29700 sample\nDone 29800 sample\nDone 29900 sample\nDone 30000 sample\nDone 30100 sample\nDone 30200 sample\nDone 30300 sample\nDone 30400 sample\nDone 30500 sample\nDone 30600 sample\nDone 30700 sample\nDone 30800 sample\nDone 30900 sample\nDone 31000 sample\nDone 31100 sample\nDone 31200 sample\nDone 31300 sample\nDone 31400 sample\nDone 31500 sample\nDone 31600 sample\nDone 31700 sample\nDone 31800 sample\nDone 31900 sample\nDone 32000 sample\nDone 32100 sample\nDone 32200 sample\nDone 32300 sample\nDone 32400 sample\nDone 32500 sample\nDone 32600 sample\nDone 32700 sample\nDone 32800 sample\nDone 32900 sample\nDone 33000 sample\nDone 33100 sample\nDone 33200 sample\nDone 33300 sample\nDone 33400 sample\nDone 33500 sample\nDone 33600 sample\nDone 33700 sample\nDone 33800 sample\nDone 33900 sample\nDone 34000 sample\nDone 34100 sample\nDone 34200 sample\nDone 34300 sample\nDone 34400 sample\nDone 34500 sample\nDone 34600 sample\nDone 34700 sample\nDone 34800 sample\nDone 34900 sample\nDone 35000 sample\nDone 35100 sample\nDone 35200 sample\nDone 35300 sample\nDone 35400 sample\nDone 35500 sample\nDone 35600 sample\nDone 35700 sample\nDone 35800 sample\nDone 35900 sample\nDone 36000 sample\nDone 36100 sample\nDone 36200 sample\nDone 36300 sample\nDone 36400 sample\nDone 36500 sample\nDone 36600 sample\nDone 36700 sample\nDone 36800 sample\nDone 36900 sample\nDone 37000 sample\nDone 37100 sample\nDone 37200 sample\nDone 37300 sample\nDone 37400 sample\nDone 37500 sample\nDone 37600 sample\nDone 37700 sample\nDone 37800 sample\nDone 37900 sample\nDone 38000 sample\nDone 38100 sample\nDone 38200 sample\nDone 38300 sample\nDone 38400 sample\nDone 38500 sample\nDone 38600 sample\nDone 38700 sample\nDone 38800 sample\nDone 38900 sample\nDone 39000 sample\nDone 39100 sample\nDone 39200 sample\nDone 39300 sample\nDone 39400 sample\nDone 39500 sample\nDone 39600 sample\nDone 39700 sample\nDone 39800 sample\nDone 39900 sample\nDone 40000 sample\nDone 40100 sample\nDone 40200 sample\nDone 40300 sample\nDone 40400 sample\nDone 40500 sample\nDone 40600 sample\nDone 40700 sample\nDone 40800 sample\nDone 40900 sample\nDone 41000 sample\nDone 41100 sample\nDone 41200 sample\nDone 41300 sample\nDone 41400 sample\nDone 41500 sample\nDone 41600 sample\nDone 41700 sample\nDone 41800 sample\nDone 41900 sample\nDone 42000 sample\nDone 42100 sample\nDone 42200 sample\nDone 42300 sample\nDone 42400 sample\nDone 42500 sample\nDone 42600 sample\nDone 42700 sample\nDone 42800 sample\nDone 42900 sample\nDone 43000 sample\nDone 43100 sample\nDone 43200 sample\nDone 43300 sample\nDone 43400 sample\nDone 43500 sample\nDone 43600 sample\nDone 43700 sample\nDone 43800 sample\nDone 43900 sample\nDone 44000 sample\nDone 44100 sample\nDone 44200 sample\nDone 44300 sample\nDone 44400 sample\nDone 44500 sample\nDone 44600 sample\nDone 44700 sample\nDone 44800 sample\nDone 44900 sample\nDone 45000 sample\nDone 45100 sample\nDone 45200 sample\nDone 45300 sample\nDone 45400 sample\nDone 45500 sample\nDone 45600 sample\nDone 45700 sample\nDone 45800 sample\nDone 45900 sample\nDone 46000 sample\nDone 46100 sample\nDone 46200 sample\nDone 46300 sample\nDone 46400 sample\nDone 46500 sample\nDone 46600 sample\nDone 46700 sample\nDone 46800 sample\nDone 46900 sample\nDone 47000 sample\nDone 47100 sample\nDone 47200 sample\nDone 47300 sample\nDone 47400 sample\nDone 47500 sample\nDone 47600 sample\nDone 47700 sample\nDone 47800 sample\nDone 47900 sample\nDone 48000 sample\nDone 48100 sample\nDone 48200 sample\nDone 48300 sample\nDone 48400 sample\nDone 48500 sample\nDone 48600 sample\nDone 48700 sample\nDone 48800 sample\nDone 48900 sample\nDone 49000 sample\nDone 49100 sample\nDone 49200 sample\nDone 49300 sample\nDone 49400 sample\nDone 49500 sample\nDone 49600 sample\nDone 49700 sample\nDone 49800 sample\nDone 49900 sample\nDone 50000 sample\nDone 50100 sample\nDone 50200 sample\nDone 50300 sample\nDone 50400 sample\nDone 50500 sample\nDone 50600 sample\nDone 50700 sample\nDone 50800 sample\nDone 50900 sample\nDone 51000 sample\nDone 51100 sample\nDone 51200 sample\nDone 51300 sample\nDone 51400 sample\nDone 51500 sample\nDone 51600 sample\nDone 51700 sample\nDone 51800 sample\nDone 51900 sample\nDone 52000 sample\nDone 52100 sample\nDone 52200 sample\nDone 52300 sample\nDone 52400 sample\nDone 52500 sample\nDone 52600 sample\nDone 52700 sample\nDone 52800 sample\nDone 52900 sample\nDone 53000 sample\nDone 53100 sample\nDone 53200 sample\nDone 53300 sample\nDone 53400 sample\nDone 53500 sample\nDone 53600 sample\nDone 53700 sample\nDone 53800 sample\nDone 53900 sample\nDone 54000 sample\nDone 54100 sample\nDone 54200 sample\nDone 54300 sample\nDone 54400 sample\nDone 54500 sample\nDone 54600 sample\nDone 54700 sample\nDone 54800 sample\nDone 54900 sample\nDone 55000 sample\nDone 55100 sample\nDone 55200 sample\nDone 55300 sample\nDone 55400 sample\nDone 55500 sample\nDone 55600 sample\nDone 55700 sample\nDone 55800 sample\nDone 55900 sample\nDone 56000 sample\nDone 56100 sample\nDone 56200 sample\nDone 56300 sample\nDone 56400 sample\nDone 56500 sample\nDone 56600 sample\nDone 56700 sample\nDone 56800 sample\nDone 56900 sample\nDone 57000 sample\nDone 57100 sample\nDone 57200 sample\nDone 57300 sample\nDone 57400 sample\nDone 57500 sample\nDone 57600 sample\nDone 57700 sample\nDone 57800 sample\nDone 57900 sample\nDone 58000 sample\nDone 58100 sample\nDone 58200 sample\nDone 58300 sample\nDone 58400 sample\nDone 58500 sample\nDone 58600 sample\nDone 58700 sample\nDone 58800 sample\nDone 58900 sample\nDone 59000 sample\nDone 59100 sample\nDone 59200 sample\nDone 59300 sample\nDone 59400 sample\nDone 59500 sample\nDone 59600 sample\nDone 59700 sample\nDone 59800 sample\nDone 59900 sample\nDone 60000 sample\nDone 60100 sample\nDone 60200 sample\nDone 60300 sample\nDone 60400 sample\nDone 60500 sample\nDone 60600 sample\nDone 60700 sample\nDone 60800 sample\nDone 60900 sample\nDone 61000 sample\nDone 61100 sample\nDone 61200 sample\nDone 61300 sample\nDone 61400 sample\nDone 61500 sample\nDone 61600 sample\nDone 61700 sample\nDone 61800 sample\nDone 61900 sample\nDone 62000 sample\nDone 62100 sample\nDone 62200 sample\nDone 62300 sample\nDone 62400 sample\nDone 62500 sample\nDone 62600 sample\nDone 62700 sample\nDone 62800 sample\nDone 62900 sample\nDone 63000 sample\nDone 63100 sample\nDone 63200 sample\nDone 63300 sample\nDone 63400 sample\nDone 63500 sample\nDone 63600 sample\nDone 63700 sample\nDone 63800 sample\nDone 63900 sample\nDone 64000 sample\nDone 64100 sample\nDone 64200 sample\nDone 64300 sample\nDone 64400 sample\nDone 64500 sample\nDone 64600 sample\nDone 64700 sample\nDone 64800 sample\nDone 64900 sample\nDone 65000 sample\nDone 65100 sample\nDone 65200 sample\nDone 65300 sample\nDone 65400 sample\nDone 65500 sample\nDone 65600 sample\nDone 65700 sample\nDone 65800 sample\nDone 65900 sample\nDone 66000 sample\nDone 66100 sample\nDone 66200 sample\nDone 66300 sample\nDone 66400 sample\nDone 66500 sample\nDone 66600 sample\nDone 66700 sample\nDone 66800 sample\nDone 66900 sample\nDone 67000 sample\nDone 67100 sample\nDone 67200 sample\nDone 67300 sample\nDone 67400 sample\nDone 67500 sample\nDone 67600 sample\nDone 67700 sample\nDone 67800 sample\nDone 67900 sample\nDone 68000 sample\nDone 68100 sample\nDone 68200 sample\nDone 68300 sample\nDone 68400 sample\nDone 68500 sample\nDone 68600 sample\nDone 68700 sample\nDone 68800 sample\nDone 68900 sample\nDone 69000 sample\nDone 69100 sample\nDone 69200 sample\nDone 69300 sample\nDone 69400 sample\nDone 69500 sample\nDone 69600 sample\nDone 69700 sample\nDone 69800 sample\nDone 69900 sample\nDone 70000 sample\nDone 70100 sample\nDone 70200 sample\nDone 70300 sample\nDone 70400 sample\nDone 70500 sample\nDone 70600 sample\nDone 70700 sample\nDone 70800 sample\nDone 70900 sample\nDone 71000 sample\nDone 71100 sample\nDone 71200 sample\nDone 71300 sample\nDone 71400 sample\nDone 71500 sample\nDone 71600 sample\nDone 71700 sample\nDone 71800 sample\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tên tệp để lưu\nfile_name = \"/kaggle/working/gt_pred_yolo5_resnet.txt\"\n\n# Ghi danh sách vào tệp văn bản\nwith open(file_name, 'w') as file:\n    for sublist in data:\n        # Chuyển mỗi phần tử trong sublist thành chuỗi\n        sublist[7] = round(sublist[7],2)\n        sublist = [str(item) for item in sublist]\n        # Ghi sublist vào tệp, phân tách các phần tử bằng dấu phẩy\n        file.write(','.join(sublist) + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:55:25.870616Z","iopub.execute_input":"2024-03-21T15:55:25.870913Z","iopub.status.idle":"2024-03-21T15:55:26.167368Z","shell.execute_reply.started":"2024-03-21T15:55:25.870889Z","shell.execute_reply":"2024-03-21T15:55:26.166620Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}